{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datathon/dataset/product_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the Image descriptions of all images using LLaVA.\n",
    "We did so in Colab but due to the time constraint we described only 220 images. (prompt used in the LLaVA.prompt file)\n",
    "The descriptions of those images are in the img_descriptions.txt file with the format\n",
    "{image name}||{image description}\n",
    "\n",
    "Now we will load those descriptions and work only with those 220 peaces. With a little bit more resources you can easily process all images using an accelerator.\n",
    "\n",
    "Overall, descriptions are accurate and drastically more informative that the data of the dataset, but there are some inconsistencies.\n",
    "Results can be improved by fine-tuning a LLaVA model on clothes description.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.csv\", \"r\") as f :\n",
    "    content = f.read()\n",
    "    lines = content.splitlines()\n",
    "    \n",
    "    pairs = []\n",
    "    for l in lines :\n",
    "        split = l.split(\"||\")\n",
    "        pairs.append((split[0], split[1]))\n",
    "\n",
    "df[\"img_description\"] = None\n",
    "print(pairs)\n",
    "\n",
    "for p in pairs :\n",
    "    df[\"img_description\"] = np.where(df[\"des_filename\"] == \"datathon/images/\" + p[0],\n",
    "                                p[1], df[\"img_description\"])\n",
    "    \n",
    "df.dropna(inplace=True, subset=[\"img_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate rich product descriptions with the OpenAI text-davinci-003 model. This way we can get the best of all the data provided in the dataset and the generated by LLaVA.\n",
    "\n",
    "The following code takes the restrictions of the OpenAI free api limit of 3 CPM and only the first 192 items were processed due to the 200 CPD. With less than 3$ you could describe really efficiently all the 9000 peaces.\n",
    "\n",
    "The prompt used can be also seen in Description.prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from time import sleep\n",
    "\n",
    "llm = OpenAI(model=\"davinci-instruct-beta\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Item Description:\\n{img_description}\\nGive me a paragraph describing the item, it MUST contain only essential information related to these topics: Type of clothing item {des_product_type}. Sex (Man, woman, unisex or child.) {des_sex} . Material it is made of {des_fabric}. Weather where this item is worn. Colors the item has (just the colors of the clothing item) {des_color_specification_esp}. Type of cut  (e. g. type of sleeves, width, neck, collar...). Print it has (e.g. plain, stripes, animal print, geometrical), with more specific print details. Any other details like a text (say what is written), drawing (say what is drawn), pockets (and its place) or other details like a lace, add them too. Any non relevant information should not appear. Write everything in a paragraph, not a list.\\nYour Item Description:\\n\",\n",
    "    input_variables=[\"img_description\", \"des_product_type\", \"des_sex\", \"des_fabric\", \"des_color_specification_esp\"]\n",
    ")\n",
    "\n",
    "descriptions = []\n",
    "for _, r in df.iterrows() :\n",
    "        prompt = prompt_template.format(img_description=r[\"img_description\"], des_product_type=r[\"des_product_type\"], des_sex=r[\"des_sex\"], des_fabric=r[\"des_fabric\"], des_color_specification_esp=r[\"des_color_specification_esp\"])\n",
    "        output = llm(prompt)\n",
    "        descriptions.append(output)\n",
    "        sleep(20)\n",
    "        \n",
    "df[\"description\"] = descriptions + [None] * (len(df) - len(descriptions))\n",
    "df.dropna(subset=[\"description\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will embed all the peaces based on the description and add them to a Chroma DB using langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "loader = DataFrameLoader(df, page_content_column=\"description\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "chroma = Chroma(collection_name=\"items\", persist_directory=\"./chroma_index\", embedding_function=embedding)\n",
    "chroma.add_documents(documents=documents)\n",
    "chroma.persist()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
